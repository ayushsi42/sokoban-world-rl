World Model-Based Reinforcement Learning for Complex Puzzle Solving: A DreamerV3 Approach to Sokoban
Abstract
This research proposal presents a comprehensive investigation into applying world model-based reinforcement learning, specifically DreamerV3, to solve complex logical puzzle environments. We focus on Sokoban as the primary testbed due to its requirement for long-term planning, spatial reasoning, and sequential decision-making under combinatorial complexity. The project aims to evaluate whether learned world models can effectively simulate and plan ahead in environments requiring sophisticated forward planning, contributing to the broader understanding of model-based RL in discrete, logic-intensive domains.
1. Introduction and Motivation
1.1 Research Context
DreamerV3 is a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration, representing the state-of-the-art in world model-based reinforcement learning. However, most evaluations have focused on continuous control tasks and Atari-style games. This research addresses a critical gap by investigating its effectiveness in discrete, logic-heavy puzzle environments.
1.2 Problem Statement
Traditional model-free RL methods struggle with Sokoban due to:

Sparse rewards: Solutions require long sequences of correct actions
Combinatorial explosion: State space grows exponentially with puzzle size
Irreversible actions: Many moves cannot be undone, requiring careful planning
Long-term dependencies: Success depends on actions taken many steps ago

1.3 Research Questions

Can DreamerV3's learned world model accurately simulate the discrete dynamics of Sokoban?
How does imagination-based planning compare to model-free approaches in puzzle domains?
What architectural modifications improve world model accuracy for discrete environments?
How does performance scale with puzzle complexity and planning horizon?

2. Technical Implementation Framework
2.1 Environment Setup
Primary Environment: Sokoban

Repository: mpSchrader/gym-sokoban - The most mature Gymnasium-compatible implementation
Alternative: Griddly framework with GDY-Sokoban-v0 for additional configurability

Environment Specifications:
python# Core environment configuration
import gymnasium as gym
import gym_sokoban

env_configs = {
    'Sokoban-v0': {
        'dim_room': (10, 10),        # Room dimensions
        'max_steps': 120,            # Maximum episode length
        'num_boxes': 4,              # Number of boxes to push
        'reset': True,               # Allow environment reset
        'render_mode': 'rgb_array'   # For DreamerV3 visual processing
    },
    'Sokoban-v1': {
        'dim_room': (7, 7),          # Smaller for initial training
        'max_steps': 50,
        'num_boxes': 2
    }
}
Benchmark Environments

Sokoban variants: Different room sizes (7x7, 10x10, 13x13)
Box pushing complexity: 2-6 boxes per puzzle
Procedural generation: Random level generation to prevent overfitting

2.2 DreamerV3 Implementation
Core Algorithm

Base Implementation: Official DreamerV3 repository (danijar/dreamerv3)
PyTorch Alternative: NM512/dreamerv3-torch for easier modification

Key Components:

World Model: RSSM (Recurrent State Space Model) with discrete latent states
Actor-Critic: Policy and value functions trained on imagined trajectories
Representation Learning: Encoder-decoder architecture for visual observations

python# DreamerV3 configuration for Sokoban
dreamer_config = {
    'world_model': {
        'rssm_units': 512,
        'rssm_layers': 1,
        'encoder': 'cnn',           # Convolutional encoder for grid observations
        'decoder': 'cnn',           # Reconstruction decoder
        'discrete_states': 32,      # Categorical latent states
        'classes': 32              # Classes per categorical
    },
    'actor_critic': {
        'actor_layers': [256, 256],
        'critic_layers': [256, 256],
        'discount': 0.99,
        'lambda_': 0.95            # GAE parameter
    },
    'training': {
        'batch_size': 16,
        'sequence_length': 64,     # Long sequences for planning
        'imagination_horizon': 15,  # Steps ahead to imagine
        'model_lr': 1e-4,
        'actor_critic_lr': 3e-4
    }
}
2.3 Experimental Architecture
Phase 1: Baseline Implementation (Months 1-2)
bash# Environment setup
pip install gymnasium gym-sokoban torch torchvision
git clone https://github.com/NM512/dreamerv3-torch.git
cd dreamerv3-torch

# Modify for discrete environments
python setup.py develop

# Training pipeline
python train.py \
    --env Sokoban-v1 \
    --steps 1000000 \
    --eval_every 10000 \
    --seed 42
Phase 2: Architecture Modifications (Months 3-4)
Discrete State Enhancements:

Categorical Latents: Replace continuous latents with discrete categorical distributions
Gumbel-Softmax: Differentiable sampling for discrete world model training
State Abstraction: Learn higher-level representations of game states

pythonclass DiscreteRSSM(nn.Module):
    def __init__(self, categories=32, classes=32):
        super().__init__()
        self.categories = categories
        self.classes = classes
        # Implementation details for categorical RSSM
        
    def forward(self, obs, action, prev_state):
        # Discrete state transition with Gumbel-Softmax
        logits = self.transition(prev_state, action)
        state = F.gumbel_softmax(logits, hard=True)
        return state
Phase 3: Advanced Planning (Months 5-6)
Enhanced Imagination:

Hierarchical Planning: Multi-scale temporal abstraction
Value-Guided Search: Use learned value function to guide imagination
Uncertainty-Aware Planning: Incorporate model uncertainty in decisions

2.4 Evaluation Framework
Quantitative Metrics

Success Rate: Percentage of solved puzzles
Sample Efficiency: Steps to reach performance thresholds
Planning Quality: Correlation between imagined and actual trajectories
Model Accuracy: Reconstruction loss and prediction error

Qualitative Analysis

Visualization: Render imagined trajectories vs. actual gameplay
Strategy Analysis: Identify learned planning strategies
Failure Mode Analysis: Categorize types of planning failures

python# Evaluation pipeline
def evaluate_dreamer_sokoban(model, env, num_episodes=100):
    metrics = {
        'success_rate': 0,
        'avg_steps': 0,
        'planning_accuracy': 0,
        'imagination_quality': 0
    }
    
    for episode in range(num_episodes):
        obs = env.reset()
        done = False
        steps = 0
        
        while not done:
            # Generate imagined trajectory
            imagined_states = model.imagine(obs, horizon=15)
            
            # Select action based on imagination
            action = model.act(obs, imagined_states)
            obs, reward, done, info = env.step(action)
            steps += 1
            
        metrics['success_rate'] += info.get('solved', 0)
        metrics['avg_steps'] += steps
    
    return {k: v/num_episodes for k, v in metrics.items()}
3. Advanced Research Components
3.1 World Model Enhancements
Discrete Dynamics Modeling
Traditional DreamerV3 uses continuous latent states, but Sokoban's discrete nature suggests categorical latents might be more appropriate:
pythonclass CategoricalWorldModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.state_categories = 64  # Number of discrete state categories
        self.state_classes = 32     # Classes per category
        
    def observe(self, obs):
        # Encode observation to categorical distribution
        logits = self.encoder(obs)
        return F.gumbel_softmax(logits.view(-1, self.state_categories, self.state_classes))
    
    def transition(self, state, action):
        # Predict next state distribution
        return self.dynamics_net(torch.cat([state, action], dim=-1))
Hierarchical State Abstraction
Implement multi-resolution state representations to capture both local (box positions) and global (room connectivity) information:

Low-level states: Individual cell contents
Mid-level states: Box-goal relationships
High-level states: Overall puzzle structure

3.2 Planning Improvements
Value-Guided Imagination
Instead of uniform sampling during imagination, use the learned value function to guide exploration:
pythondef guided_imagination(self, obs, horizon=15):
    states = [obs]
    values = []
    
    for step in range(horizon):
        current_state = states[-1]
        
        # Generate possible actions
        action_candidates = self.actor.sample_multiple(current_state, k=5)
        
        # Evaluate each candidate
        candidate_values = []
        for action in action_candidates:
            next_state = self.world_model.predict(current_state, action)
            value = self.critic(next_state)
            candidate_values.append(value)
        
        # Select action with highest expected value
        best_action = action_candidates[np.argmax(candidate_values)]
        next_state = self.world_model.predict(current_state, best_action)
        
        states.append(next_state)
        values.append(max(candidate_values))
    
    return states, values
3.3 Curriculum Learning Integration
Progressive Difficulty
Start with simple puzzles and gradually increase complexity:
pythoncurriculum_stages = [
    {'room_size': (5, 5), 'boxes': 1, 'steps': 25},    # Stage 1: Trivial
    {'room_size': (7, 7), 'boxes': 2, 'steps': 50},    # Stage 2: Simple  
    {'room_size': (10, 10), 'boxes': 3, 'steps': 80},  # Stage 3: Medium
    {'room_size': (13, 13), 'boxes': 4, 'steps': 120}, # Stage 4: Complex
]

def curriculum_scheduler(agent_performance, current_stage):
    if agent_performance['success_rate'] > 0.8:
        return min(current_stage + 1, len(curriculum_stages) - 1)
    return current_stage
4. Experimental Design and Timeline
4.1 Phase 1: Foundation (Months 1-2)
Objectives: Establish baseline performance and validate implementation

Set up DreamerV3 with Sokoban environment
Implement basic training pipeline
Collect baseline performance metrics

Deliverables:

Working DreamerV3-Sokoban integration
Initial performance benchmarks
Comparison with model-free baselines (PPO, A3C)

4.2 Phase 2: Architecture Innovation (Months 3-4)
Objectives: Develop discrete-optimized world model variants

Implement categorical RSSM
Test hierarchical state representations
Evaluate planning quality improvements

Deliverables:

Enhanced world model architectures
Comparative analysis of continuous vs. discrete representations
Planning visualization tools

4.3 Phase 3: Advanced Planning (Months 5-6)
Objectives: Implement sophisticated planning strategies

Value-guided imagination
Uncertainty-aware decision making
Multi-horizon planning strategies

Deliverables:

Advanced planning algorithms
Comprehensive evaluation framework
Strategy analysis and interpretation

4.4 Phase 4: Scaling and Analysis (Months 7-8)
Objectives: Test scalability and conduct thorough analysis

Large-scale puzzle evaluation
Failure mode analysis
Transfer learning experiments

Deliverables:

Scalability analysis
Comprehensive failure mode taxonomy
Transfer learning results

5. Expected Contributions and Novelty
5.1 Technical Contributions

Discrete World Models: Novel architecture adaptations for categorical state spaces
Planning Quality Metrics: New evaluation frameworks for imagination-based planning
Hierarchical State Abstraction: Multi-scale representations for puzzle domains
Value-Guided Imagination: Improved planning through value function integration

5.2 Scientific Impact
Primary Hypothesis: Learned world models can effectively simulate discrete, logic-intensive environments when augmented with appropriate architectural modifications.
Secondary Hypotheses:

Categorical latent representations outperform continuous ones in discrete domains
Value-guided imagination improves planning quality over uniform sampling
Hierarchical state abstraction enables better generalization across puzzle variants

5.3 Broader Implications
This research contributes to understanding model-based RL in several key areas:

Discrete Dynamics: Extending world models beyond continuous control
Logical Reasoning: Demonstrating planning in combinatorial domains
Sample Efficiency: Reducing data requirements through better simulation

6. Resource Requirements and Budget
6.1 Computational Resources
Training Requirements:

GPU: NVIDIA RTX 4090 or equivalent (24GB VRAM minimum)
CPU: 16+ cores for parallel environment simulation
RAM: 64GB for large replay buffers
Storage: 1TB SSD for experiment data

Estimated Training Time:

Baseline experiments: ~200 GPU hours
Architecture variants: ~500 GPU hours
Advanced planning: ~300 GPU hours
Total: ~1000 GPU hours

6.2 Software Dependencies
bash# Core dependencies
torch>=2.0.0
torchvision>=0.15.0
gymnasium>=0.28.1
numpy>=1.21.0
matplotlib>=3.5.0

# Environment-specific
gym-sokoban
griddly

# Experiment management
wandb
tensorboard
hydra-core

# Analysis tools
seaborn
plotly
jupyter
6.3 Cost Estimation
ComponentCost (8 months)Cloud Computing (AWS p3.8xlarge)$8,000Software Licenses$500Data Storage$200Total$8,700
7. Risk Analysis and Mitigation
7.1 Technical Risks
Risk 1: DreamerV3 may not adapt well to discrete environments

Mitigation: Implement multiple architectural variants (categorical, hierarchical)
Fallback: Compare with discrete-native algorithms (MuZero, AlphaZero)

Risk 2: Computational requirements exceed available resources

Mitigation: Implement efficient training strategies, use smaller puzzle variants
Fallback: Focus on specific architectural components rather than full-scale evaluation

Risk 3: World model accuracy insufficient for effective planning

Mitigation: Develop model-uncertainty quantification and planning robustness
Fallback: Hybrid model-free/model-based approach

7.2 Research Risks
Risk 1: Limited novelty in results

Mitigation: Focus on architectural innovations and thorough analysis
Contingency: Pivot to transfer learning or multi-task scenarios

Risk 2: Negative results (DreamerV3 fails in discrete domains)

Value: Negative results are scientifically valuable and publishable
Extension: Analyze failure modes to guide future algorithm development

8. Success Metrics and Evaluation
8.1 Quantitative Targets
Primary Success Criteria:

Baseline Performance: >60% success rate on 7x7 Sokoban puzzles
Scalability: Maintain >40% success rate on 10x10 puzzles
Sample Efficiency: Achieve target performance in <500K environment steps
Planning Quality: Imagined trajectories correlate >0.7 with optimal solutions

Secondary Metrics:

Model accuracy (reconstruction loss <0.1)
Training stability (consistent improvement over 1M steps)
Generalization (>50% performance on unseen puzzle configurations)

8.2 Qualitative Assessment
Strategy Analysis: Manual inspection of learned strategies

Does the agent exhibit forward planning behavior?
Can it discover complex multi-step strategies?
How does strategy complexity correlate with puzzle difficulty?

Imagination Quality: Visual analysis of imagined trajectories

Do imagined sequences resemble valid gameplay?
Can the model predict consequences of irreversible actions?
How accurately does imagination represent spatial relationships?

9. Conclusion and Future Work
This research proposal outlines a comprehensive investigation into world model-based reinforcement learning for complex puzzle domains. By focusing on Sokoban as a challenging testbed, we aim to advance understanding of how learned world models can handle discrete, logic-intensive environments requiring sophisticated forward planning.
The project's contributions span both technical innovations (discrete world model architectures) and scientific insights (effectiveness of imagination-based planning in combinatorial domains). Success would demonstrate the broader applicability of world model approaches beyond their traditional continuous control domains.
Future Directions:

Extension to other puzzle domains (sliding puzzles, maze navigation)
Multi-agent collaborative puzzle solving
Integration with large language models for symbolic reasoning
Real-world applications in logistics and planning domains

Implementation Timeline Summary
PhaseDurationKey MilestonesFoundation2 monthsWorking baseline, initial resultsArchitecture2 monthsEnhanced world models, discrete adaptationsAdvanced Planning2 monthsSophisticated planning algorithmsAnalysis2 monthsComprehensive evaluation, paper writing
This project represents a significant step toward understanding the capabilities and limitations of world model-based RL in discrete, logical reasoning domains, with clear practical implications for AI planning and decision-making systems.

Repository Structure (to be created):
dreamer-sokoban/
├── src/
│   ├── dreamer/          # Modified DreamerV3 implementation
│   ├── environments/     # Sokoban environment wrappers  
│   ├── planning/         # Advanced planning algorithms
│   └── analysis/         # Evaluation and visualization tools
├── experiments/          # Experiment configurations
├── notebooks/           # Analysis notebooks
├── docs/               # Documentation and results
└── requirements.txt    # Dependencies